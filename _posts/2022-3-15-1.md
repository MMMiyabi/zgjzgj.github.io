---
layout:     post
title:      "Learning to Pre-train Graph Neural Networks"
subtitle:   ""
date:       2022-3-15 14:00:00
author:     "zgj"
catalog: true
header-style: text
tags: 论文阅读
---

学习预训练图神经网络

### Abstract

图神经网络已经成为图表征学习的实际标准，预先训练的GNN可以从上游任务中学习到可转移的知识，然后继续在下游任务中学习，从而提高预测水平

传统的GNN预训练方法分两步：

1. 在丰富的无标签数据集上进行训练
2. 在下游有标签数据集上进行微调

本文认为这两步的目标存在很大的分歧，为了消除这种分歧，本文提出了L2P-GNN，一种自监督预训练的GNN网络。

其中的关键思路是尝试在预训练的过程中学会把先验知识微调成可转移的形式。

为了将全局信息和局部信息都编码到先验知识中，该模型在节点级和图级上设计了双重适应机制。

### Introduction

消除这种分歧面临两个关键挑战：

1. 如何缩小两个目标间的差距
2. 如何在无标签数据中同时保留节点级和图级的信息

为了应对这两个挑战，本文提出了L2P-GNN。

对于第一个挑战，L2P-GNN在预训练过程中模拟微调过程，这种学习形式可以被认为为**元学习**的一种形式。

对于第二个挑战，本文提出一个具有**双重适应机制**的**自监督策略**，以完全自我监督的方式在节点和图层面上进行预训练。一方面，节点级适应将节点对的连接性作为自我监督的信息，从而学习一个可转移的先验来编码局部图的属性。另一方面，图层面的适应是为了保留图中的全局信息，其中一个子结构应该在表示空间中接近整个图。

本文的主要贡献如下：

- 这是首次尝试探索学习预训练GNN的方法，它缓解了预训练和微调目标之间的分歧，并为预训练GNN提供了一个新视角。
- 我们提出了一个完全自我监督的GNN预训练策略，用于节点和图层面的表示。
- 我们建立了一个新的大规模书目图数据来预训练GNN，并在不同领域的两个数据集上进行了广泛的实证研究。实验结果表明，我们的方法持续且明显地优于现有的技术水平。

### Related Word

### Learning to Pre-train: Motivation and Overview

在这一节分析并证明这种分歧的存在。

#### GNNS

节点迭代更新公式：
![](https://i.vgy.me/9QrBXr.png)

图迭代更新公式：

![](https://i.vgy.me/z2cUWM.png)

h是整个图的表征，H是节点矩阵，READOUT是简单的池化操作，如sum,max,mean-pooling等

#### Conventional GNN Pre-training

预训练的目标：

![](https://i.vgy.me/K9AMoF.png)

微调的目标：

![](https://i.vgy.me/TgPQOg.png)

#### Learning to Pre-train GNNs

预训练和微调的分歧在于θ0的优化没有考虑到对下游任务的任何适应。

本文提出将预训练过程阶段化，并且模拟下游的微调过程。

具体操作是，从预训练的数据集中抽取用来模拟微调任务的训练集和测试集。在预训练和模拟微调后，在模拟测试集上的误差最小。

![](https://i.vgy.me/fnUzK6.png)

其中，下式是微调参数：

![](https://i.vgy.me/XT2UiV.png)

本文认为，这样产生的预训练的输出，θ0，不是为了直接优化任何特定任务的训练或测试数据。相反，θ0是最优的，因为它允许快速适应一般的新任务。

#### Connection to other works

本文的方式是元学习的一种，是MAML的改版

### Proposed Method

在下文中，我们将介绍我们的方法L2P-GNN。我们首先介绍了一个用于在MAML环境下学习图结构的自监督基础GNN模型，然后是我们在节点和图层面的双重适应，旨在模拟预训练过程中的微调。

#### Self-supervised Base Model

L2P-GNN的核心是学习预训练GNN的概念，以弥补预训练和微调过程之间的差距。具体来说，我们的方法可以被表述为MAML的一种形式。为此，我们定义了一个任务，即从局部和全局的角度捕捉图中的结构和属性。然后，元学习的先验可以适应一个新的任务或图。

##### Task Construction

从预训练数据集中抽取一批数据作为一个任务Tg

任务Tg又被分为多个子任务Tig

每一个子任务的数据被分为支持集和查询集，Sig和Qig，支持集和查询集不相交

![](https://i.vgy.me/iatC40.png)

Sig和Qig包含从图的边分布中随机抽样的边，而且互相排斥

> where the support S c G and query Qc G contain edges randomly sampled from the edge distribution pE of the graph, and they are mutually exclusive. In essence, child tasks incorporate the local connectivity between node pairs in a graph, and they fuse into a parent task TG = (SG, QG) to facilitate a graph-level view

子任务包含图的局部连通性，组成父任务就促进图级的视图

##### Base GNN Model

十分典型的GNN模型，节点表征和分子表征都是常见的模式

节点的负样本是与u没有连边的v`

![](https://i.vgy.me/ZRWtxI.png)

图的负样本是随机移位一些维度的样本

![](https://i.vgy.me/wYreNZ.png)

为了捕获节点级和层级的信息，整个图的损失函数如下

![](https://i.vgy.me/UwSXMX.png)

#### Dual Adaptation

**节点级适应**：用几个模拟支持集来进行梯度下降算法更新参数

![](https://i.vgy.me/nPuiSk.png)

**图级适应**：梯度下降算法来更新

![](https://i.vgy.me/IsgqcU.png)

#### Optimization of Transferable Prior

根据适应性优化后的参数θ`，在查询集上优化更新θ

![](https://i.vgy.me/V80BDH.png)

完整算法如下：

![](https://i.vgy.me/S2yi7i.jpg)

### Experiments

数据集：

![](https://i.vgy.me/Igvm3Y.png)

#### Performance Comparison

![](https://i.vgy.me/01WkAf.png)

从表中可以观察到：

1. 在所有的GNNs架构中，L2P-GNN预训练模型获得最佳结果；
2. 无标签数据的预训练显然有助于下游任务；
3. 在下游任务中出现负转移（如GAT上的EdgePred）,原因可能是预训练策略学习了与下游任务无关的信息，损害了预训练GNNs的泛化能力。负迁移导致预训练模型的适用性和可靠性

### Conclusion

本文提出一种自监督的GNNs 预训练策略L2P-GNN。在传统的预训练策略中，训练前的目标和微调目标之间存在偏差，导致了次优的预训练的GNN模型。L2P-GNN构造预训练步骤来模拟下游任务的微调过程，从而直接优化预训练模型对下游任务的快速适应性。在节点和图层次上，L2P-GNN具有双重适应性，利用无标签图数据的内在结构作为自我监督，同时学习局部和全局表示。实验表明，L2P-GNN的性能明显优于最先进的技术，并有效地缩小了预训练和微调之间的差距。
