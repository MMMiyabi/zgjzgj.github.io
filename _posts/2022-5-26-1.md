---
layout:     post
title:      "How little data do we need for patient-level prediction?"
subtitle:   ""
date:       2022-5-26 21:30:00
author:     "zgj"
catalog: true
header-style: text
tags: 论文阅读
---

### ABSTRACT

目的：确认用于训练模型的数据集的合理大小，平衡模型的有效性和复杂度

方法：对81个预测问题建模了17248个模型，通过学习曲线评估数据量对模型效果和复杂度（变量数量）的影响。

结论：在大多数情况下，使用一部分小数据集就能产生和使用全部数据集相同的效果，但会使模型复杂度显著降低



### Background and significance

本文认为启发式的EPV规则存在一些问题：

- 结论源自受限的样本量
- 只建立少数模型，缺乏普遍性
- 规则过于简单
- 需要事先了解变量的数量，对于变量很多的问题不可取

作为对比，本文使用学习曲线来探究样本量与模型性能的关系，并且对81个预测问题建立了17248个模型

**什么是EPV？**

Events per variable，即每个变量的事件数，事件表示目标值中数量最少的那一类

在预测问题不变时，事件的百分比变化等价于样本量的百分比变化

>例如调查胃癌发病与 3 种生活因素（X1代表不良饮食习惯，X2 代表喜吃卤食和盐渍食物，X3 代表精神状况）的关系，若胃癌患者占的比例为20%，那么当假EPV=10 时，由于有 3 个协变量，所以所需胃癌患者例数为10×3=30，总共需要的样本量（胃癌患者和健康对照）为 30÷20%=150 例

根据 A Simulation Study of the Number of Events per Variable in Logistic Regression Analysis 中的结论，EPV ≥ 10时，建立的模型能反映真实的规律

不过这个结论只是一个经验法则，并不是绝对真理



### Materials and methods

**数据集**

![](https://i.vgy.me/EablPM.png)

**预测及拟合曲线**

通过分层抽样来取每个子集，以保持与完整训练集相同的结果率，样本大小的建议将以事件的数量为标准。子集的大小从100个事件到20,000个事件以100为单位增加

使用 Levenberg-Marquardt 算法拟合学习曲线，为了画出学习曲线，我们将模型在测试集上的表现（AUROC）与用于训练的事件数量作对比。

![](https://i.vgy.me/ioQL1R.png)

为了绘制模型复杂度曲线，我们将测试集上的变量数量（用LASSO进行选择）与用于训练的事件数量作了对比。

**对足量样本的判断**

判断样本数是否足够有两种标准，一种基于斜率，一种基于阈值，本文采取基于阈值的标准

基于阈值的标准是基于在**所有可用样本上训练的模型**与在**减少的样本量上训练的模型**之间的性能差异

![](https://i.vgy.me/hhFdDX.png)

### Results

**拟合情况**

预测问题的最大AUROCs的中位数（四分位数范围）为0.742（0.700 - 0.790）

如可以预料的那样，在小数量事件的情况下，真实的学习曲线更容易不稳定，这种不稳定随着事件数量的增加而减小

![](https://i.vgy.me/wym9qB.png)

![](https://i.vgy.me/afqrZG.png)

**足量事件数**

拟合曲线周围的数据点，方差很大，表明对于不同的数据集和预测目标，足量的事件数差别很大

第二张图显示，对于0.001、0.005、0.01和0.02的阈值，实现的中位数减少（四分位数范围）分别为9.5%（6.7%-13.6%）、37.3%（28.4%-48.0%）、58.5%（46.1%-68.8），和78.5%（67.8%-85.1%）

![](https://i.vgy.me/Pc3cui.png)

![](https://i.vgy.me/TDhIwx.png)

**模型复杂度**

模型复杂性的大幅降低是在样本量减少的基础上实现的

与事件数量的情况类似，曲线周围的数据点方差很大，表明对于不同的数据集和预测目标，能够减少的变量数差别很大

完整模型的中位数（四分位数范围）为588（356 - 858）个预测因子，而阈值为0.02的模型有181（120 - 265）个预测因子

对于阈值0.001、0.005、0.01、0.02，我们实现的模型复杂性的中位数减少（四分位数范围）分别为8.6%（3.8%-14.6%）、32.2%（22.6%-38.4%）、48.2%（38.7%-56.7%）、68.3%（58.9%-75.7%）

![](https://i.vgy.me/32uYI0.png)

![](https://i.vgy.me/leuZyp.png)

### Conclusion

本文为通过减少样本量降低模型复杂度提供了经验性的指导：

- 在要求不高的情况下，如果研究者接受0.01的阈值，对于一个有20,000个事件的给定数据集，可以减少的样本量（中位数）为73%，对应的复杂度（中位数）将减少59%
- 在要求很高的情况下，建议生成一条特定的学习曲线。以本文的研究为例，对于一个阈值为0.01和具有20,000个事件的数据集，样本量的减少可能达到84%

但也要注意：

- 本文虽然建立了很多个模型，但使用的数据集其实只有3个

- 本文使用的算法模型局限于 logistic 回归
- 本文学习曲线使用的指标为逆幂律拟合曲线
- 本文的研究是内插型的，不一定适用于外推