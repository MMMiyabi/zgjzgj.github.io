---
layout:     post
title:      "A Survey on Meta Learning“
subtitle:   ""
date:       2022-3-21 15:30:00
author:     "zgj"
catalog: true
header-style: text
tags: 论文阅读
---

本文基于《A Survey of Deep Meta-Learning》

### 什么是元学习

通常在机器学习里，我们会使用某个场景的大量数据来训练模型；然而当场景发生改变，模型就需要重新训练。但是对于人类而言，一个小朋友成长过程中会见过许多物体的照片，某一天，当Ta（第一次）仅仅看了几张狗的照片，就可以很好地对狗和其他物体进行区分

元学习Meta Learning，含义为学会学习，即learn to learn，就是带着这种对人类这种“学习能力”的期望诞生的。Meta Learning希望使得模型获取一种“学会学习”的能力，使其可以在获取已有“知识”的基础上快速学习新的任务

在其最广泛的意义上，它涵盖了所有利用先前的学习经验以更快地学习新任务的系统

### 元学习与其它一些概念的区别

- 一般的机器学习：通过数据集学习到一个映射关系，使经验误差最小
- 元学习：通过外部任务集，学习到一些元知识，能帮助更快更好的学习新的任务

- 迁移学习：迁移学习与元学习确实有很大的联系。不过你大可以将迁移学习想到这样一种情况，即在一个大数据集上预训练一个模型，并在一个较小的数据集上微调所学的权重。迁移学习只有一个模型，不管是源任务还是目标任务都在这个模型上更新参数，元学习则不同，每一个训练任务可能有单独的模型，单独计算loss，元模型就根据这些loss来更新参数

- 多任务学习：元学习和多任务学习其实没有太多类似的地方，多任务学习是指一个模型被联合训练以在多个固定任务中表现良好。在元学习中，出现多个任务的原因，只是因为元学习器要从尽量多的任务中学习更容易泛化的知识，其目的是找到一个能够快速学习新的（以前未见过的）任务的模型

![](https://i.vgy.me/2Bhbw7.png)

### 元学习的过程

![](https://i.vgy.me/rJfQ8e.png)

元学习的过程可以分为两个阶段，元训练阶段和元测试阶段。我们谈论原理时，是在谈论元训练阶段。在元训练阶段，一般会有多个训练任务。在元学习中，我们对每个训练任务的训练集和测试集另起名字，叫做支持集和查询集

在元训练阶段，对一个任务，模型通过支持集学习并预测查询集，其目标是使预测的误差最小。整个元模型，通过在多个任务上进行这个步骤，慢慢累积元知识

在元测试阶段，元知识就固定不变了，但模型（如果有继承下来的模型的话）仍是可以更新的

### 元学习的分类

使用2017年Vinyals提出的分类法，将元学习分为：基于度量的，基于模型的，基于优化的

![](https://i.vgy.me/V6mw6c.png)

#### 基于度量的

![](https://i.vgy.me/2QTL8B.png)

基于度量的技术的目标是获得一个优秀的特征空间

基于度量的学习技术旨在学习一个相似性核，或者等价的注意机制kθ（以θ为参数），它接受两个输入x1和x2，并输出它们的相似性分数。分数越大说明相似度越高。然后，通过将x与我们知道其真实标签yi的例子输入xi进行比较，可以对新输入x进行类别预测。

- 匹配网络

为支持集与查询集分别训练一个嵌入函数，测试集嵌入后根据与支持集的匹配度进行预测

![](https://i.vgy.me/HY1yhW.png)

- 原型网络

与匹配网络类似，但原型网络不是计算新输入和支持集中的例子之间的相似性，而是只将新输入与类别原型（中心点）进行比较

![](https://i.vgy.me/zUjppd.png)

- 关系网络

把分析相似度的工作交给神经网络

![](https://i.vgy.me/6RbxkO.png)

#### 基于模型的

对于“基于模型的”这五个字怎么理解，我到现在还不是很确定。或许可以理解为这种方法要对模型进行精细化设计，或许可以理解为这种方法能学习到快速建模的能力（类似于为了考试背答案）

各种基于模型的技术。尽管有明显的差异，但它们都建立在**任务内部化**的概念上。也就是说，任务的信息被吸收进模型之中，更具体的说，任务的信息在基于模型的系统的状态中被处理和表示。然后，这些信息可以被用来帮助预测

![](https://i.vgy.me/5Qf5qU.png)

- 循环元学习

以RNN模型为骨架，任务和样本按时间序列输入，从而将之前看到过的信息嵌入到模型之中

![](https://i.vgy.me/GW2Ksu.png)

- 记忆增强神经网络（MANNs）

记忆增强型神经网络的关键思想是使神经网络能够在外部记忆的帮助下快速学习。主网络（与存储器交互的递归神经网络）逐渐积累跨任务的知识，而外部存储器允许快速适应特定任务

有点像我们为了准备考试，既会慢慢学习知识，也会直接背真题答案

当从记忆中读取时，实际是产生记忆矩阵M中存储的标签的加权平均值，对那些与当前类别有较大相似度的类别给予更大的权重

![](https://i.vgy.me/tniOUM.png)

- 元网络

元网络直接把学习分为快慢两部分。元网络分为两个不同的子系统（都由神经网络组成），基学习器和元学习器。基础学习器负责执行任务，并向元学习器提供元信息，如损失梯度。然后，元学习器可以为自己和基础学习器计算快速的特定任务权重

![](https://i.vgy.me/J7VtnA.png)

![](https://i.vgy.me/NCobAA.png)

#### 基于优化的

与前两种方法相比，基于优化的技术对元学习采取了不同的观点。他们明确地对快速学习进行优化。大多数基于优化的技术是通过将元学习作为一个双层优化问题来实现的。在内部层面，一个基础学习者使用一些优化策略（比如梯度下降）进行特定任务的更新。在外部层面，则去优化模型快速学习的能力

![](https://i.vgy.me/TynUtf.png)

- MAML

MAML在深度元学习领域获得了极大的关注，其优点有：

1. 简单性（只需要两个超参数）
2. 普遍适用性
3. 强大的性能

![](https://i.vgy.me/o2GA1s.png)

### 总结

|            | 特点                   | 优点                           | 缺点                                                         |
| ---------- | ---------------------- | ------------------------------ | ------------------------------------------------------------ |
| 基于度量的 | 计算输入相似性         | 简单有效                       | 不适用于非监督学习                                           |
| 基于模型的 | 在模型中嵌入任务与状态 | 内部灵活，泛化性比基于度量的高 | 在监督学习中有时不如基于度量的方法（图神经网络），泛化性能不如基于优化的方法 |
| 基于优化的 | 两层优化               | 泛化性能很强                   | 计算成本高                                                   |

