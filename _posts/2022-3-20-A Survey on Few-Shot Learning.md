---
layout:     post
title:      "Generalizing from a Few Examples: A Survey on Few-Shot Learning"
subtitle:   ""
date:       2022-3-20 14:30:00
author:     "zgj"
catalog: true
header-style: text
tags: 论文阅读
---

这是之前读的一篇综述，现在整理了一下放上来

## Abstract

- FSL的核心问题是 经验误差最小化不可靠
- FSL的分类：
  - 数据：使用先验知识增加监督经验
  - 模型：使用先验知识缩小假设空间
  - 算法：使用先验知识改变优化算法



## Introduction

介绍本文工作：

- 给出FSL的正式定义，包括现有工作，目标，解决方法
- 列出FSL的问题
- 指出FSL的核心问题是经验误差最小化的不可靠性，为改进提供启示
- 将FSL算法进行了分类，讨论了每一类的优缺点
- 展望未来的方向

## Overview

2.1 FSL的定义和分类

- 小样本学习也可分为小样本回归，小样本分类，小样本强化学习

- 小样本学习的几个应用场景：

  - 学会像人类一样学习
  - 从稀有的例子中去学习
  - 减少数据收集和计算的成本

  在这些场景中，用传统的机器学习方法总是会失败的，必须结合先验知识，先验知识可以是任何关于未见样本的信息

  one-shot learning 

  zero-shot learning

2.2 相关的机器学习问题

- 弱监督学习：

弱监督学习又可分为半监督学习和主动学习，半监督学习指只有一小部分样本有标签，而大部分样本没有标签。弱监督学习只包含回归和分类。

如果FSL使用的先验数据只有无标记的样本，并且是一个回归或分类任务时，就是弱监督学习。

- 不平衡学习：

不平衡指数据的分布不平衡，比如某个标签的样本数量很少。不平衡学习会产生所有标签的可能性，而FSL则是用那少量的样本进行训练和测试，并将其它样本作为先验知识。

- 迁移学习：

迁移学习在FSL中经常被使用

- 元学习

2.3 核心问题

![在这里插入图片描述](https://i.vgy.me/TJRA7K.png)

模型与真理之间的误差，可以分为结构误差和经验误差，结构误差是指受到模型的限制，能实现的最好情况与真实情况的误差，经验误差是指受到数据的限制，能达到的最好情况与真实情况的误差。

![](https://i.vgy.me/4Ip904.png)

在FSL中，我们如何解决这个问题？

![image-20211215103724747](https://i.vgy.me/YWFzdr.png)

数据：增强数据集

模型：减小假设空间

算法：优化搜索策略

![image-20211215104006054](https://i.vgy.me/dZXgEW.png)

## Data

数据增强：使用先验知识对数据进行变换。

![image-20211215105734695](https://i.vgy.me/xtHxoF.png)

3.1 训练集变换

将原始训练集通过变换转变成新的训练集

3.2 弱标签或无标签数据集的变换

从有标签的样本中训练出一个SVM，然后对无标签的样本进行分类或回归，然后将预测正确的样本加入到原始样本中进行。还有诸如标签传播等方法。

3.3 相似数据集的变换

聚合相似数据集，聚合权重基于相似性测量。

这种做法可能会产生误差，因此，生成对抗网络用来产生无差别的合成

3.4 总结

选择使用哪种增强策略取决于应用。有时，目标任务（或类别）存在大量的弱监督或未标记的样本，但由于收集注释数据的成本较高和/或计算成本较高（这与第2.1节介绍的第三种情况相对应），所以倾向于使用少数次学习。在这种情况下，人们可以通过转换弱标记或无标记数据集的样本来进行增量。当大规模的无标签数据集难以收集，但少许拍摄的类有一些类似的类，人们可以从这些类似的类中转换样本。如果只有一些学习到的转化器而不是原始样本，可以通过转化Dtrain中的原始样本来进行增量。
一般来说，通过增强Dtrain来解决FSL问题是简单明了的，容易理解。
通过利用目标任务的先验信息，对数据进行增强。另一方面，通过数据增强来解决FSL问题的弱点是，增强策略往往是以临时的方式为每个数据集量身定做的，不能轻易用于其他数据集（尤其是其他领域的数据集）。最近，AutoAugment[27]被提出来解决这个问题，它可以自动学习深度网络训练的增强策略。除此之外，现有的方法主要是为图像设计的，因为生成的图像可以很容易地被人类视觉评估。相比之下，文本和音频涉及语法和结构，更难生成。最近的一次尝试是在[144]中报告了对文本使用数据增强的情况。

## Model

如果在小样本数据上训练一个简单的模型，通常来说，模型的优化空间更小，可以缓解小样本学习的问题，可是模型太过于简单（例如简单的线性回归），根本无法有效表征复杂的数据特征；然而如果使用较为复杂的模型，则将会面临过拟合（Overfitting）问题。因此，基于模型的小样本学习方法，旨在通过先验知识来进一步缩减假设空间 H \mathcal{H}H 的大小，即使使用较为复杂的模型，也可以在很少的经验样本的基础上快速靠近最优解。

![image-20211215112701231](https://i.vgy.me/C1ZdWt.png)

4.1 多任务学习

可以类比之前讲过的迁移学习。把其它任务中学到的参数作为先验知识，对小样本集进行训练。对参数的约束可分为参数共享和参数捆绑：

参数共享：

![image-20211215114534402](https://i.vgy.me/Bym4vb.png)

参数捆绑：把成对参数的差作为正则化项进行惩罚。

![image-20211215114850571](https://i.vgy.me/EVnsC4.png)

4.2 嵌入学习

把高维空间的样本映射到低维空间，使相似的样本更加靠近

嵌入学习包括三个主要部分：训练集的嵌入函数g，测试集的嵌入函数f，相似度评估函数s

嵌入函数通过先验知识来学习，f和g虽然可以使用相同的函数，但不同会有更好的准确率

根据f和g的参数是否在不同的任务中发生变化，可以将嵌入学习分为以下三大类

![image-20211216092947407](https://i.vgy.me/IpK4bH.png)

4.2.1 特定任务的嵌入模型

4.2.2 不变任务的嵌入模型

![image-20211216093526023](https://i.vgy.me/mbKgA0.png)

4.3 外部记忆（External Memory）学习

![image-20211216095714281](https://i.vgy.me/mJBbMg.png)

4.4 生成模型

![image-20211216102539575](https://i.vgy.me/PZgcdc.png)

4.5 小结

当存在类似的任务或辅助任务时，可以用多任务学习来约束少见的任务的H。然而，请注意，需要对所有的任务进行联合训练。因此，当一个新的少量任务到来时，整个多任务模型必须再次被训练，这可能是昂贵而缓慢的。此外，D和Dc的大小不应具有可比性，否则，少量任务可能会被许多样本的任务所淹没。
当存在一个包含足够多的各类样本的大规模数据集时，人们可以使用嵌入学习方法。这些方法将样本映射到一个良好的嵌入空间，在这个空间中，不同类别的样本可以很好地分开，因此需要一个较小的H˜。然而，当少见的任务与其他任务关系不密切时，它们可能效果不好。此外，对如何混合任务的不变信息和特定任务信息的更多探索是有帮助的。
当记忆网络可用时，通过在记忆之上训练一个简单的模型（如分类器），它可以很容易地被用于FSL。通过使用精心设计的更新规则，人们可以有选择地保护内存插槽。这种策略的弱点是它会产生额外的空间和计算成本，这些成本随着存储器的大小而增加。因此，目前的外部存储器有一个有限的大小。
最后，当人们想执行除FSL外的生成和重建等任务时，可以使用生成模型。它们从其他数据集中学习先验概率p(z;γ)，从而将H减少到一个较小的H˜。学习到的生成模型也可以用来生成样本，用于数据的增强。然而，生成模型方法有很高的推理成本，而且比确定性模型更难推导出

## Algorithm

算法指的是在假设空间中搜索最佳参数的算法，样本量较小时，可能无法搜素到最佳的参数。

根据先验知识影响获取最佳参数的方式，将此类方法分为三种：

![image-20211216105316638](https://i.vgy.me/fQs8KX.png)

5.1 改进参数

认为从大训练集中获得的参数能反映出一些普遍的结构性质

5.1.1 通过正则化的方式对参数进行微调

![image-20211216110216779](https://i.vgy.me/8LpNBj.png)

调整时避免过拟合：

- 早停
- 选择性更新部分参数
- 同时更新一组参数
- 使用模型回归网络：模型回归网络捕捉到task-agnostic的转变，它将在少数例子上训练得到的参数值映射到在大量样本上训练得到的参数值

5.1.2 聚合参数

![image-20211216111411722](https://i.vgy.me/lWhF57.png)

5.1.3 增加新的参数，并对之前的参数进行微调

![image-20211216111940430](https://i.vgy.me/9uQrEr.png)

5.2 改进元学习器的参数

![image-20211216112101551](https://i.vgy.me/8h59ag.png)

通过元学习改进θ

5.3 学习优化器（ Learning the Optimizer）

学习出一个优化器，能直接输出更新，而不是通过梯度下降去寻找优化

![image-20211216112859336](https://i.vgy.me/pVHxqq.png)

5.4 小结

通过使用现有的θ0作为初始化，这些方法通常需要较低的计算成本来获得一个好的假设h∈H，学习的重点是细化这些现有的参数。然而，由于θ0是从与当前任务不同的任务中学习的，这种策略可能会为了速度而牺牲精度。
另外两种策略则依赖于元学习。通过从一组相关任务中学习，金属学习的θ0可以更接近新任务Tt的特定参数jt。元学习者的学习搜索步骤可以直接指导学习算法。换句话说，元学习器充当了一个优化器。然而，诸如如何在不同的粒度（如动物的粗粒度分类与狗类的细粒度分类）或不同的数据源（如图像与文本）进行元学习等重要问题[131]仍未解决。从这个角度来看，元学习和多任务是相似的，因此也有一个关于如何避免负面转移的问题[28] 。

## Future Works

## Conclusion

Few-Shot Learning（FSL）的目标是弥合人工智能和人类学习之间的差距。它可以通过纳入先验知识来学习只包含少数有监督信息的新任务。FSL作为人工智能的测试平台，使罕见情况的学习成为可能，或者有助于减轻工业应用中收集大规模监督数据的负担。在这份调查报告中，我们对FSL进行了全面而系统的回顾。我们首先正式定义了FSL，并讨论了FSL与相关学习问题（如弱监督学习、不平衡学习、转移学习和元学习）的关联和区别。然后，我们指出FSL的核心问题是不可靠的经验风险最小化器，它使得FSL难以学习。对核心问题的理解有助于将不同的工作按照它们如何利用先验知识解决核心问题分为数据、模型和算法：数据增强了FSL的监督经验，模型约束了FSL的假设空间，使之更小，而算法则改变了在给定假设空间中寻找最佳假设的搜索策略。在每个类别中，都深入讨论了利弊，并提出了一些总结和见解。为了激发FSL的未来研究，我们还提供了问题设置、技术、应用和理论方面的可能方向，以供探讨。
