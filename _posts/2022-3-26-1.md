---
layout:     post
title:      "Deep learning with small datasets: using autoencoders to address limited datasets in construction management"
subtitle:   ""
date:       2022-3-26 11:30:00
author:     "zgj"
catalog: true
header-style: text
tags: 论文阅读
---

### Abstract

本文用不完全的、稀疏的、深度的和变异的自动编码器被作为数据增强和生成合成数据的方法来研究.

平均而言，自动编码器为地下和高架数据集提供了7.2%和11.5%的模型得分改进

所有自动编码器的MAE和RMSE也都比较低。地下和高架数据集的平均误差分别为22.9%和56.5%

本文还为从业者提供了解决大数据集访问受限的方法，一种从数据中的非线性关联中提取洞察力的可视化方法，以及一种改善数据隐私和利用类似的合成数据实现敏感数据共享的方法

### Introduction

这些方法（数据增强，数据合成）大多只适用于平移不变的数据（如图像）；而不适合平移变化的数据（如金融数据）。

本文通过探索自动编码器（无监督的机器学习模型）和变异自动编码器（生成模型）作为处理建筑管理中有限的数据变异数据集的工具，来解决这一问题。

本研究的目标是 (i) 定义一种方法，使用自动编码器来增强建筑管理活动的变量数据集；以及(ii) 比较不同类型的自动编码器在数据增强方面的性能。

### Background

#### 数据增强

数据增强的三种方式：线性变换、插值和变形、基于概率的方法

数据增强的基本原理是通过在原始数据集中创建重复的实例，并对其进行转换或扭曲以创建略有不同的实例，来增加用于训练的数据集。

线性变换常用在图像分类任务，因为图像具有变换不变性。图像上的简单变换，如旋转、反射、缩放和挤压，可以通过对图像施加位移场并将每个像素的原始位置改为新的位置来产生。

插值和变形法为新生成的数据集引入非线性失真和随机性。∆x(x, y) = rand(-1, +1);
∆x(x, y) = rand(-1, +1) 其中，rand（-1, +1）是一个从均匀分布中抽取的介于-1和+1之间的随机数。

数据增强的概率方法寻求在训练数据集中产生缺失数据。以概率的方式生成训练集实例中的变量分布。然后，增强的数据集通常与具有概率特性的深度学习模型结合使用，如受限玻尔兹曼机和深度信念网络

#### 合成数据

用类似事物的模拟产生的数据来增加原始训练数据集。

合成数据方法的一个特点是，他们在生成的数据中加入扭曲和噪音，使合成数据与现实生活中的数据相似。

### 自动编码器实现数据增强

与图像相反，金融数据并不具有平移不变性。因此，传统的线性和非线性变换和失真方法不能使用

自动编码器是无监督的神经网络，它试图重构输入，并作为输出

一个自动编码器的典型结构通常由两部分组成，即编码器和解码器。编码器将原始输入（x）压缩成一个新的表示，称为 "潜在矢量 "或 "压缩表示"。解码器根据输入特征之间的相关性，将该表征解压为新的重建输入（x）。网络可以通过最小化原始输入和重构输入之间的损失函数L(x, x ′)来训练。在节点中必须使用非线性激活函数，并且必须随机地初始化权重，以便网络不会简单地重现准确的输入。

#### 不完全自动编码器

不完全自动编码器制约了隐藏层的节点数量，以限制流经网络的信息量。这样一来，在训练过程中，网络将学会从压缩的表示中重建原始输入的基本特征。

#### 去噪自动编码器

去噪自动编码器是一种不完全的自动编码器，它在将原始输入输入到自动编码器之前，故意将噪声随机地添加到原始输入中。添加噪声可以避免自动编码器简单地将输入复制到输出而不学习训练数据的结构。添加噪声的方法通常是将一些输入值随机转换为空值。

#### 稀疏自动编码器

稀疏自动编码器在训练过程中随机限制活跃节点的数量，以创造信息流的瓶颈，而不是减少隐藏层的节点数量

#### 变分自动编码器

变分自动编码器是估计原始训练数据的概率密度函数的生成模型。与其他类型的自动编码器的主要区别是，变分自动编码器学习代表原始数据的概率分布的参数。常规自动编码器的主要限制是，输入被编码的潜空间和编码向量所在的潜空间可能不连续。如果潜空间是不连续的，解码器可能会重建一个不现实的输入。变异自动编码器通过定义一个基于原始数据概率分布的潜空间来解决这个问题。在这个意义上，变异自动编码器具有生成的特征，可以被视为生成合成数据的方法，而不是数据增强的方法。

### 方法和材料

在本研究中，只使用两个数据集中的**连续变量**

地下数据集由27个项目组成，而架空数据集由41个项目组成。这些数据集被分成了训练集和测试集。地下和高架数据集所使用的训练和测试数据分割分别为21/6和31/10。

请注意，所有的变量都被规范化为0-1的范围。

然后，使用四种类型的自动编码器对两个训练集进行增强，即：（1）不完全自动编码器，（2）稀疏自动编码器，（3）深度自动编码器，和（4）变异自动编码器

![](https://i.vgy.me/KAPZIw.png)

上述过程对每种类型的自动编码器重复20次。每一次，训练集都被一个更高的数据增量系数所扩大。也就是说，在第一次增强迭代中，训练集的大小增加了一倍；在第二次迭代中，它增加了三倍，以此类推，直到20次迭代。使用四种类型的自动编码器增加训练集的结果进行了比较。
使用DNN回归器、DTR和RFR，将没有增强的训练集的结果作为基线。

![](https://i.vgy.me/GfOxNv.png)

用R方值，MAE和RMSE（被规范化到0-1）来衡量模型的好坏

### 结果

增量系数定义了训练数据集中包含的额外数据量。对于这两个数据集，自动编码器比初始模型产生更好的结果。随着增量因子的增加，可以观察到性能的增长趋势。然而，这种性能的提高有可能归因于模型对数据的过度拟合，因为MAE和RMSE随着增量因子的提高而增加

对于地下和高架数据集，MAE和RMSE最初随着增量因子的增加而减少，之后开始增加，直到它们达到与没有增量的初始模型中获得的相似值。这表明，在这种情况下，数据增量在一定程度上提供了好处，之后，额外的数据会导致过度拟合。

![](https://i.vgy.me/ktkquV.png)

![](https://i.vgy.me/kKneCF.png)

![](https://i.vgy.me/9guKMQ.png)

### 讨论

在使用神经网络回归器时，使用自动编码器进行数据增强是一个潜在的好方法。所有被调查的自动编码器都提供了明显更好的模型得分和更低的MAE和RMSE误差。

然而，在使用其他非深度学习方法时，使用自动编码器进行数据增强并没有提供明显的改进。

由自动编码器创建的压缩表示可以提供关于数据集中变量之间的潜在非线性关系的有用见解。

压缩表示法也可用于研究变量是否相关。

这项研究对知识的贡献是：（1）它提出了一种用于变换变量数据的数据增强技术。许多技术已经被开发出来，用于转换不变的数据，有助于提高深度学习模型的性能。这项研究表明，自动编码器可以是一个很好的选择，用于变换变量数据的数据增强。(2) 它提出了经验性的证据，表明当没有大型数据集时，自动编码器可以帮助解决泛化问题。(3)它提供的证据表明，使用自动编码器的数据增强对深度学习回归模型如DNN是有用的，但对其他回归模型如RFR和SVR则没有作用。最后，（4）它表明，由变异自动编码器产生的压缩表征可以提供有价值的见解，可以发掘出数据集中变量之间的潜在关系。

### 结论

