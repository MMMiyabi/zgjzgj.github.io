---
layout:     post
title:      "衡量两个概率分布差异性的指标"
subtitle:   ""
date:       2022-4-11 20:16:00
author:     "zgj"
catalog: true
header-style: text
tags: 论文阅读
---

### 信息熵

![](https://i.vgy.me/abD9iE.png)



信息熵的三个性质：

1. 单调性，发生概率越高的事件，其携带的信息量越低；
2. 非负性，信息熵可以看作为一种广度量，非负性是一种合理的必然；
3. 累加性，即多随机事件同时发生存在的总不确定性的量度是可以表示为各事件不确定性的量度的和，这也是广度量的一种体现。

![](https://i.vgy.me/hngeuB.png)

### KL散度

![](https://i.vgy.me/rgZoDY.png)

相对熵又称KL散度,如果我们对于同一个随机变量 x 有两个单独的概率分布 P(x) 和 Q(x)，我们可以使用 KL 散度（Kullback-Leibler (KL) divergence）来衡量这两个分布的差异。

在机器学习中，P往往用来表示样本的真实分布，Q用来表示模型所预测的分布，那么KL散度就可以计算两个分布的差异，也就是Loss损失值。

从KL散度公式中可以看到Q的分布越接近P（Q分布越拟合P），那么散度值越小，即损失值越小。

因为对数函数是凸函数，所以KL散度的值为非负数。

有时会将KL散度称为KL距离，但它并不满足距离的性质：

1. KL散度不是对称的；
2. KL散度不满足三角不等式。

### JS散度

![](https://i.vgy.me/Bf33Cu.png)

JS散度度量了两个概率分布的相似度，基于KL散度的变体，解决了KL散度非对称的问题。一般地，JS散度是对称的，其取值是0到1之间。

### Wasserstein距离

![](https://i.vgy.me/qe5Mw6.png)

KL散度和JS散度度量的问题：

如果两个分配P,Q离得很远，完全没有重叠的时候，那么KL散度值是没有意义的，而JS散度值是一个常数。这在学习算法中是比较致命的，这就意味这这一点的梯度为0。梯度消失了。

Π(P1,P2)是P1和P2分布组合起来的所有可能的联合分布的集合。对于每一个可能的联合分布γ，可以从中采样(x,y)∼γ得到一个样本x和y，并计算出这对样本的距离||x−y||，所以可以计算该联合分布γ下，样本对距离的期望值E(x,y)∼γ[||x−y||]。在所有可能的联合分布中能够对这个期望值取到的下界infγ∼Π(P1,P2)E(x,y)∼γ[||x−y||]就是Wasserstein距离。

直观上可以把E(x,y)∼γ[||x−y||]理解为在γ这个路径规划下把土堆P1挪到土堆P2所需要的消耗。而Wasserstein距离就是在最优路径规划下的最小消耗。所以Wesserstein距离又叫Earth-Mover距离。

**Wasserstein距离相比KL散度、JS散度的优越性在于，即便两个分布没有重叠，Wasserstein距离仍然能够反映它们的远近；**而JS散度在此情况下是常量，KL散度可能无意义。WGAN本作通过简单的例子展示了这一点。考虑如下二维空间中的两个分布![[公式]](https://www.zhihu.com/equation?tex=P_1)和![[公式]](https://www.zhihu.com/equation?tex=P_2)，![[公式]](https://www.zhihu.com/equation?tex=P_1)在线段AB上均匀分布，![[公式]](https://www.zhihu.com/equation?tex=P_2)在线段CD上均匀分布，通过控制参数![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta)可以控制着两个分布的距离远近。

![](https://i.vgy.me/RKwly9.png)

此时容易得到（读者可自行验证）

![[公式]](https://www.zhihu.com/equation?tex=KL%28P_1+%7C%7C+P_2%29+%3D+KL%28P_1+%7C%7C+P_2%29+%3D+%5Cbegin%7Bcases%7D+%2B%5Cinfty+%26+%5Ctext%7Bif+%24%5Ctheta+%5Cneq+0%24%7D+%5C%5C+0+%26+%5Ctext%7Bif+%24%5Ctheta+%3D+0%24%7D+%5Cend%7Bcases%7D)（突变）

![[公式]](https://www.zhihu.com/equation?tex=JS%28P_1%7C%7CP_2%29%3D+%5Cbegin%7Bcases%7D+%5Clog+2+%26+%5Ctext%7Bif+%24%5Ctheta+%5Cneq+0%24%7D+%5C%5C+0+%26+%5Ctext%7Bif+%24%5Ctheta+-+0%24%7D+%5Cend%7Bcases%7D)（突变）

![[公式]](https://www.zhihu.com/equation?tex=W%28P_0%2C+P_1%29+%3D+%7C%5Ctheta%7C)（平滑）

KL散度和JS散度是突变的，要么最大要么最小，**Wasserstein距离却是平滑的**，如果我们要用梯度下降法优化![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta)这个参数，前两者根本提供不了梯度，Wasserstein距离却可以。类似地，在高维空间中如果两个分布不重叠或者重叠部分可忽略，则KL和JS既反映不了远近，也提供不了梯度，**但是Wasserstein却可以提供有意义的梯度**。

![](https://i.vgy.me/WEqxND.jpg)

