---
layout:     post
title:      "A Review of Deep Transfer Learning and Recent Advancements"
subtitle:   ""
date:       2022-4-7 14:24:00
author:     "zgj"
catalog: true
header-style: text
tags: 论文阅读
---

### Abstract

深度迁移学习能解决没有足够有标记训练集以及在边缘设备上运行深度学习模型等问题

本文回顾了深度迁移学习（DTL）的定义和分类

### Introduction

在本文中，首先回顾了DTL的定义，然后是DTL的分类法。然后，对DTL的一些实际研究进行了列举、归类和总结。此外，还回顾了DTL的两个实验性评估及其结论。

### Deep Learning

### Deep Transfer Learning (DTL)

与随机初始化相比，基于训练过的网络，即使是非常遥远的数据集，初始化这些权重也能提高训练性能。

DTL与半监督学习不同，因为在DTL中，源数据集和目标数据集可以有不同的分布，只是相互之间有关系，而在半监督学习中，源数据和目标数据都来自同一个数据集，只是目标集没有标签。

DTL也不等同于多视图学习，因为多视图学习使用两个或多个不同的数据集来提高一个任务的质量。

DTL与多任务学习不同。最根本的区别是，在多任务学习中，任务之间利用相互联系来相互促进，知识转移在相关任务之间同时发生。相反，在DTL中，目标领域是重点，并且已经从源数据中获得了目标数据的知识，它们不需要相关或同时发挥作用。

### From Transfer Learning to Deep Transfer Learning, Taxonomy

DTLs**可以根据数据集被标记的情况分为三种**：

- 直推式（transductive）：只有源数据集被标记
- 归纳式（inductive）：源数据集和目标数据集都被标记
- 无监督式：源数据集和目标数据集都没有标记

DTLs也可以通过应用方法进行另一种分类：

- 基于实例：直接从源数据的实例学习，应用不同的加权政策
- 基于特征/映射：将源数据和目标数据映射到共同的特征空间
- 基于参数/网络：使用模型中学习到的参数（知识），使用冻结/微调/添加进行调整
- 基于关系/对抗：提取可转移的特征，或者使用在源领域学到的逻辑关系或规则，或者使用GAN

![](https://i.vgy.me/v5FjdN.png)

**深度迁移学习主要集中在基于网络的方法上**，基于网络的方法可以解决源数据和目标数据差距非常大的适应问题

基于网络的方法，主要就是靠微调或冻结让模型适应到目标领域中，现在一种最新的DTL技术是基于冻结一个预训练的模型，并向该模型添加新的层，以便在目标数据上进行训练。

谷歌的深度思维项目在2016年将这种技术介绍为渐进式学习/渐进式神经网络（PNNs）

### Review of Recent Advancement in DTL

这部分总结了在过去4-5年发表的关于各种任务和数据类型的深度迁移学习的研究，所有的研究大多属于三类基于网络的方法，还有一些属于对抗的方法，这些方法被命名为：

- 微调：对预训练的模型进行微调
- 冻结CNN层：冻结早期的CNN层，对全连接层进行微调
- 渐进式学习：选择预训练模型的部分或全部层并冻结使用，添加一些新层进行训练
- 基于对抗：使用对抗或关系方法从源数据和目标数据中提取可转移特征

![](https://i.vgy.me/2tOwzq.png)

### Experimental Studies in Deep Transfer Learning

本节分析了四种不同情况下的网络。(i)预训练的网络，(ii)随机初始化的网络，(iii)在源域预训练后在目标域进行微调的网络，(iv)从随机初始化中训练的网络。此外，为了描述特征重用的作用，他们使用了一个包含自然图像的源（预训练）域（IMAGENET），以及一些与自然图像的视觉相似度递减的目标（下游）域、DOMAINNET真实、DOMAINNET剪贴画、CHEXPERT（医学胸部X射线）和DOMAINNET快速绘图。

特征重用在深度迁移学习中起着关键作用，但特征重用并不是深度迁移学习成功的唯一原因，因为即使是CHEXPERT和quickdraw这样的遥远目标，仍然观察到深度迁移学习的性能提升

在所有情况下，预训练的模型都比随机初始化的模型收敛得快

他们观察到，在预训练模型中标记为不正确的数据样本和在随机初始化模型中标记为正确的数据样本大多是模糊的样本。另一方面，预训练模型标记为正确和随机初始化模型标记为不正确的大多数样本是直接的样本。这意味着预训练的模型有更强的先验性，它更难适应目标领域。

全连接层是P-T模型的关键部分，越靠近输出端的模块，重要性越高

- 所有的实验都证明转移学习优于从头开始的训练（随机初始化）
- 对于85%的目标任务，存在一个源任务，该任务在ILSVCR'12预训练中名列前茅
- 当源和目标任务在同一图像域（域内）时，转移增益最大，这甚至比源大小更重要
- 当源图像域包括目标域时，正的转移增益是可能的
- 尽管多源模型带来了良好的转移，但它们被最大的域内源（within-domain source）所超越
- 对于与源在同一图像域内的65%的目标，跨任务类型的转移会带来正的转移收益
- 正如自然预期的那样，大数据集对小数据集可以进行正向转移
- 转移效果对于小目标训练集更强，这启发了通过用一小部分目标数据测试几个模型来选择转移学习的模型

### Conclusion

基于网络的DTL方法的三个子类别是文献中最广泛使用的方法。(i) 微调，(ii) 冻结CNN层，和(iii) 渐进式学习。 这些技术已经证明了它们对于各种机器学习问题的能力和有效性。在广泛的数据集上对公开可用的预训练模型进行微调的简单性是它成为最常见的迁移学习技术的原因。

此外，渐进式学习，作为DTL中的一种最新方法，正在迅速而广泛地发展。这种方法可以说是迈向人工通用智能的坚实一步，因为它是基于不断学习的方法。