---
layout:     post
title:      "Learning from small datasets containing nominal attributes"
subtitle:   ""
date:       2022-5-4 11:13:00
author:     "zgj"
catalog: true
header-style: text
tags: 论文阅读
---



### Abstract

### 1. Introduction

小样本导致模型预测困难的原因：

- 少量的样本无法表达样本空间的真实分布
- 少量的样本无法让模型学习到必要的判断条件（决策树）

![](https://i.vgy.me/ICCaLJ.png)

![](https://i.vgy.me/jZuvuT.png)

虚拟样本生成（VSG）是一种有效的方法，其中最有名的方式就是bootstrap procedure（BP），基于其的方式，如bagging，已经寻常到我们难以察觉了。

此外，还有少数类合成的方法，最有名的是SMOTE。

但BP并不生成样本，无法弥补缺失信息；SMOTE是根据少数类的值生成样本，如果训练集过度关注少数类，可能会损害模型效果。

本文提出的VSG方法，通过数据预处理来克服小样本问题，该方法分为三步：

1. 模糊关系提取：基于M5`模型树，采用模糊技术提取连续输出和名义输入之间的模糊关系
2. 样本生成：基于概率理论，在给定输出值时，通过模糊运算将所有可能的类别值组合的可能性值汇总
3. 样本过滤：通过对可能性值的分布采用模糊的α切割来过滤掉不适当的组合

### 2. Related studies

#### M5`

一个数据预处理方法，将K个符号类别值根据其输出值转化为K-1维向量的二元表示

![](https://i.vgy.me/QfMWlU.png)

#### MTD

估计数据的样本域，并用模糊成员函数（MF）表示其概率分布

![](https://i.vgy.me/RzJkpV.png)

#### The possibility theory

交集运算

![](https://i.vgy.me/F56CBk.png)

#### The possibility assessment mechanism（PAM）

根据MF创建虚拟值：

1. 从[L,U]的均匀分布中随机抽取一个vx，并计算FX(vx)
2. 从[0,1]的均匀分布中随机抽取一个rs，作为阈值
3. 如果FX(vx) > rs，那么vx可以被接受，否则被丢弃

重复上述过程，直到得到一个可以接受的vx

#### SMOTE-NC

SMOTE-NC用 Med 表示类别值之间的距离

Med 是少数类别的数字属性的标准偏差的中位数

### 3. The proposed method

#### Definition of notations

![](https://i.vgy.me/EXc2nI.png)

m为需要的虚拟样本的数量

#### The fuzzy relation extraction

根据2.1,2.2中的方法计算样本的MF

![](https://i.vgy.me/YHK4vS.png)

![](https://i.vgy.me/nOpZ6w.png)

#### The sample generation

计算COB矩阵，即输入特征类别的所有排列组合

![](https://i.vgy.me/lNCwqL.png)

生成m个虚拟输出值，根据MF函数计算各种组合的可能性，共有m*K个可能性

这些组合加上可能性，便可作为**初步的**虚拟样本

![](https://i.vgy.me/Yy47lm.png)

#### The sample filtering

使用 α切 过滤样本

![](https://i.vgy.me/pXHXoE.png)

![](https://i.vgy.me/otLLpD.png)

#### The implementation outline

模糊关系提取：

1. 确定每维特征的所有类别及对应的输出值
2. 根据MTD计算每个类别的MF

构造COB模板：

1. 对各维特征的类别进行排列组合
2. 根据数据集的实际情况和独立性原理，计算每种组合的可能性

生成虚拟样本：

1. 用PAM生成虚拟输出值
2. 用虚拟输出值和COB构造虚拟样本
3. 构造虚拟样本的本质就是为各种排列组合计算一个对应于输出值的概率

用 α切割 过滤虚拟样本

### 4. The experimental environment

UCI数据集

10折交叉验证

平均绝对百分比误差（MAPE）

双尾t-检验检查是否存在显著性差异

对比的方法是bagging和SMOTE-NC

虚拟样本的数量被设置为原始数据集的[1,2,3,4,5]倍

α切割 的过滤参数被设置为0.7

学习算是是 BPN 和 SVR，参数设置为默认值

### 5. Experimental results and discussion

不同的数据集应该选择不同的方法

虚拟样本的作用是有限的，不应该增加的太多

![](https://i.vgy.me/8JSvjN.png)

![](https://i.vgy.me/7gGyl9.png)

